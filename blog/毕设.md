## temp

- 上下文异常：某时间的数据点值是正常的，但发生的特定时间却不寻常。

## todo

- [ ] KL 散度项
- [x] 自回归移动平均模型 ARIMA
- [ ] SVM
- [ ] 随机森林
- [ ] GAN 

## 自回归移动平均模型 ARIMA（预测）

$p$ 阶自回归（AR）：

$$
X_t = \sum_{i=1}^{p} \phi_i X_{t-i} + \epsilon_t + c
$$

其中，$\phi_i$ 是自回归系数，$\epsilon_t$ 是白噪声(均值为 0，方差为常数的独立同分布随机变量序列)，$c$ 是常数。

$q$ 阶移动平均（MA）：

$$
X_t = \epsilon_t + \sum_{i=1}^{q} \theta_i \epsilon_{t-i} + c
$$

其中，$\theta_i$ 是移动平均系数。

$1$ 阶差分（I）运算：

$$
\Delta X_t = X_t - X_{t-1}
$$

对时间序列数据进行差分处理，使其变为平稳时间序列数据。

ARIMA 模型：

$$
\begin{cases}
    \text{ARIMA}(p,d,q) & \text{模型} \\
    \Delta^d X_t = \sum_{i=1}^{p} \phi_i \Delta^d X_{t-i} + \epsilon_t + \sum_{i=1}^{q} \theta_i \epsilon_{t-i} + c & \text{ARIMA}(p,d,q)
\end{cases}
$$

确定阶数后，使用最大似然估计等方法来估计模型的参数。

预测时，可以使用滚动预测的方法，即每次预测一个时间点，然后将预测值加入到历史数据中，继续预测下一个时间点。

## 孤立森林算法（异常检测）

1. 随机选择一个特征，再随机选择一个特征值作分割点，将数据集分为两部分。
2. 重复步骤1，直到数据集中只有一个数据点或者达到指定的树的最大深度。
3. 设置一个森林中树的数量，然后重复步骤 1 和 2，构建多棵随机树。
4. 对于每个数据点，计算其在每棵树中的路径长度，然后计算平均路径长度。
5. 将所有数据点按照路径长度的平均值进行排序，然后选择路径长度较短的数据点作
为异常点。

时间复杂度为 $O(n\log n)$，高效，且对异常点和噪声点具有很好的鲁棒性。高维需降维。

## 图注意力网络 GAT

有向图 $G=(V,E)$，其中 $V$ 是节点集合，$E$ 是边集合。$e_{ij}$ 表示节点 $i$ 到节点 $j$ 的有向边。对于节点 $i$，其邻居节点集合为 $N_i$，即 $N_i=\{j|e_{ij}\in E\}$。

GAT的输入是所有节点的特征矩阵 $X\in \mathbb{R}^{F\times N}$，以及有向边集 $E$ ，其中 $N$ 是节点数，$F$ 是特征维度。输出是所有节点的特征矩阵 $Z\in \mathbb{R}^{F'\times N}$，其中 $F'$ 是输出特征维度。

图神经网络的一般形式是：

$$
Z_i = \text{map}(X_i, \text{aggregate}(N_i))
$$

其中 $Z_i$ 是节点 $i$ 的输出特征，$X_i$ 是节点 $i$ 的输入特征，$\text{map}$ 是一个映射函数，$\text{aggregate}$ 是一个聚合函数。不同的图神经网络的区别在于 $\text{map}$ 和 $\text{aggregate}$ 的具体形式。

GAT 网络利用 [自注意力机制](#自注意力机制) 对节点信息进行 $\text{aggregate}$ 。

## LSTM 和 GRU

LSTM有三个门：输入门、遗忘门、输出门。

定义各权重矩阵：

$W^{xh}$ 是输入词向量到隐藏层的，$W^{hh}$ 是隐藏层到隐藏层的，$W^{hy}$ 是隐藏层到输出层的； $W^{fx}$ 是遗忘门对输入向量的，$W^{fh}$ 是遗忘门对上次输出信息的； $W^{ix}$ 是输入门对输入向量的，$W^{ih}$ 是输入门对上次输出信息的； $W^{ox}$ 是输出门对输入向量的，$W^{oh}$ 是输出门对上次输出信息的。

更新公式：

$$
\begin{cases}
    f_t = \sigma(W^{fx}x_t + W^{fh}h_{t-1}), & \text{遗忘门} \\
    i_t = \sigma(W^{ix}x_t + W^{ih}h_{t-1}), & \text{输入门} \\
    \tilde{C}_t = \tanh(W^{xh}x_t + W^{hh}h_{t-1}), & \text{本次得到的候选传递信息(本次记忆细胞信息)} \\
    C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t, & \text{结合遗忘门的待传递信息(记忆细胞)} \\
    \\
    o_t = \sigma(W^{ox}x_t + W^{oh}h_{t-1}), & \text{输出门} \\
    h_t = o_t \odot \tanh(C_t), & \text{结合记忆细胞的输出信息}

\end{cases}
$$

流程图：（网图，符号表示和上面的公式有点不一样）

<img src="https://www.mdpi.com/information/information-14-00598/article_deploy/html/images/information-14-00598-g006.png" style="zoom: 33%;" />

---

GRU只有两个门：更新门、重置门。

定义各权重矩阵：

$W^{z}$ 是更新门对输入向量的，$W^{r}$ 是重置门对输入向量的； $W^{hz}$ 是更新门对上次输出信息的，$W^{hr}$ 是重置门对上次输出信息的。

更新公式：

$$
\begin{cases}
    z_t = \sigma(W^{z}x_t + W^{hz}h_{t-1}), & \text{更新门} \\
    r_t = \sigma(W^{r}x_t + W^{hr}h_{t-1}), & \text{重置门} \\
    \tilde{h}_t = \tanh(W^{xh}x_t + W^{hh}(r_t \odot h_{t-1})), & \text{结合重置门得到候选信息} \\
    h_t = (1-z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t, & \text{结合更新门得到输出信息}
\end{cases}
$$

## 自注意力机制

抛弃了RNN的顺序性，直接对输入序列进行处理，每个输出是所有输入的加权和：

$$
\boldsymbol{y_i} = \sum_{j=1}^{n} \alpha_{ij}\boldsymbol{x_j}
$$

其中 $\boldsymbol{x_j}$ 为第 $j$ 个输入向量；$\alpha_{ij}$ 为第 $i$ 个输入向量与第 $j$ 个输入向量之间的注意力权重。

$d$ 是特征维度，$N$ 是向量个数；$d_k$ 是查询、键、值的维度；

| 符号及其所属空间                          | 描述             |
| ----------------------------------------- | ---------------- |
| $X\in \mathbb{R}^{d\times N}$             | 输入特征矩阵     |
| $W^Q,W^K,W^V\in \mathbb{R}^{d_k\times d}$ | 权重矩阵         |
| $Q,K,V\in \mathbb{R}^{d_k\times N}$       | 查询、键、值矩阵 |
| $\alpha\in \mathbb{R}^{N\times N}$        | 注意力权重矩阵   |
| $H\in \mathbb{R}^{d_k\times N}$           | 输出特征矩阵     |

自注意力机制：（缩放点积作注意力权重）

$$
Q,K,V = (W^Q, W^K, W^V)X\\
\text{注意力权重} \alpha = \text{softmax}(\frac{K^TQ}{\sqrt{d_k}})\\
\text{输出} H = \text{Self-Attention}(Q,K,V) =  V\alpha
$$

推理时注意力只能关注到当前位置之前的位置，因此需要 mask 矩阵，将当前位置之后的位置的注意力权重设置为负无穷。所以在推理时，除了 $Q$ 之外，$K,V$ 会不断更新，且当推理到第 $t$ 个向量时， $K_{1:t-1}, V_{1:t-1}$ 并无变化，所以对 $K,V$ 进行缓存可以提高推理效率，即 KV-cache 机制。而后面的MQA、GQA、MLA，都是围绕“如何减少KV Cache同时尽可能地保证效果”这个主题发展而来的产物。

## 变分自编码器 VAE

结合了变分推断（Variational Inference）和自编码器（Auto - Encoder）的思想。

将输入数据映射到一个潜在空间中的概率分布，通常假设为多元正态分布。即：

- 编码器输出：均值向量、标准差向量
- 解码器：从均值和标准差中采样，得到潜在空间的一个点，然后通过神经网络将这个点映射回原始数据空间。

注意到直接采样不可导，采用重参数化技巧（Reparameterization Trick），即：

$$
z = \mu + \sigma \odot \epsilon
$$

其中 $\epsilon \sim N(0,1)$。

损失函数除了重构误差外，还有一个 [KL 散度项](#todo)，用于衡量编码器输出的分布与 标准正态分布(假设) 之间的差异。

## RoPE（Rotary Position Embedding）

- RoPE基于复数空间中的旋转操作来对位置信息进行编码。它将每个位置的向量表示映射到复数空间中，通过对复数进行旋转来表示不同位置之间的差异。
- **具体实现**
    - 对于一个维度为 $2d$ 的向量 $\mathbf{x}=(x_1,x_2,\cdots,x_{2d})$ ，将其拆分成两个维度为 $d$ 的子向量 $\mathbf{x}_1=(x_1,x_3,\cdots,x_{2d - 1})$ 和 $\mathbf{x}_2=(x_2,x_4,\cdots,x_{2d})$ ，然后将它们组合成一个复数向量 $\mathbf{z}=\mathbf{x}_1 + i\mathbf{x}_2$ ，其中 $i$ 是虚数单位。
    - 定义旋转矩阵 $\mathbf{R}(\theta)$ ，它对复数向量 $\mathbf{z}$ 进行旋转操作， $\mathbf{R}(\theta)=\begin{bmatrix}\cos\theta & -\sin\theta\\\sin\theta & \cos\theta\end{bmatrix}$ 。这里的 $\theta$ 是与位置 $n$ 相关的旋转角度，通常定义为 $\theta = n / 10000^{2i/d}$ ，其中 $i$ 表示维度索引， $d$ 是向量的维度。
    - 对复数向量 $\mathbf{z}$ 应用旋转矩阵 $\mathbf{R}(\theta)$ ，得到旋转后的复数向量 $\mathbf{z}'=\mathbf{R}(\theta)\mathbf{z}$ ，再将其转换回实数向量，就得到了经过RoPE编码后的位置向量。

复数向量应用旋转矩阵：

对复数向量 $\mathbf{z}=\mathbf{x}_1 + i\mathbf{x}_2$ 应用旋转矩阵 $\mathbf{R}(\theta)$ ，可以将复数向量视为二维向量：

 $\mathbf{z}'=\mathbf{R}(\theta)\mathbf{z}=\begin{bmatrix}\cos\theta & -\sin\theta\\\sin\theta & \cos\theta\end{bmatrix}\begin{bmatrix}\mathbf{x}_1\\\mathbf{x}_2\end{bmatrix}=\begin{bmatrix}\cos\theta\times\mathbf{x}_1-\sin\theta\times\mathbf{x}_2\\\sin\theta\times\mathbf{x}_1+\cos\theta\times\mathbf{x}_2\end{bmatrix}$ 

上述计算结果 $\mathbf{z}'$ 中的每一个元素都是一个复数，实部为 $\cos\theta\times\mathbf{x}_1-\sin\theta\times\mathbf{x}_2$ ，虚部为 $\sin\theta\times\mathbf{x}_1+\cos\theta\times\mathbf{x}_2$ 。

复数向量转换回实数向量：

假设经过旋转操作后的复数向量为 $\mathbf{z}' = a + bi$ ，与前面拆解类似：

- 将这两个向量重新交织组合成一个维度为 $2d$ 的实数向量 $\mathbf{x}'=(a_1,b_1,a_2,b_2,\cdots,a_d,b_d)$ ，这个 $\mathbf{x}'$ 就是经过RoPE编码后的位置向量。





## Multi-Head Latent Attention (MLA)





